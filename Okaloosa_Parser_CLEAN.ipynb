{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bmAXPq90Raq9"
   },
   "source": [
    "# Okaloosa County FL SO - ALPR Audit Parser\n",
    "\n",
    "## Instructions\n",
    "1. Run ALL cells in order\n",
    "2. Upload your PDF when prompted\n",
    "3. Download the corrected CSV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k5-iwJZVRarA"
   },
   "source": [
    "## Step 1: Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mgluztOCRarB",
    "outputId": "66097198-815d-46d8-faac-3b6169e3cdbd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o_Qfd7e_RarE"
   },
   "source": [
    "## Step 2: Upload PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "id": "osAD_WDHRarE",
    "outputId": "addf0258-5216-4a94-fa78-d634a8da935f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Found and linked: ../data/RAW-11_1_2025-12_8_2025-Okaloosa_County_FL_SO-Audit.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# '../' goes up to the 'okaloosaalpr' root\n",
    "# 'data/' goes into the data folder\n",
    "filename = \"../data/RAW-11_1_2025-12_8_2025-Okaloosa_County_FL_SO-Audit.pdf\"\n",
    "\n",
    "if os.path.exists(filename):\n",
    "    print(f\"✓ Found and linked: {filename}\")\n",
    "else:\n",
    "    print(f\"X Error: File not found at {os.path.abspath(filename)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SRBcxnETRarF"
   },
   "source": [
    "## Step 3: Extract Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tRUegBRHRarF",
    "outputId": "be4ef80c-c3f6-4eb5-e893-7cf9fcb87360",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting 809 pages...\n",
      "  100/809...\n",
      "  200/809...\n",
      "  300/809...\n",
      "  400/809...\n",
      "  500/809...\n",
      "  600/809...\n",
      "  700/809...\n",
      "  800/809...\n",
      "✓ Extracted 13,739 lines\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "\n",
    "with open(filename, 'rb') as f:\n",
    "    reader = PyPDF2.PdfReader(f)\n",
    "    num_pages = len(reader.pages)\n",
    "    print(f\"Extracting {num_pages} pages...\")\n",
    "\n",
    "    all_text = \"\"\n",
    "    for i, page in enumerate(reader.pages):\n",
    "        all_text += page.extract_text()\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"  {i + 1}/{num_pages}...\")\n",
    "\n",
    "all_lines = [line.strip() for line in all_text.split('\\n') if line.strip()]\n",
    "print(f\"✓ Extracted {len(all_lines):,} lines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDS0-0PORarG"
   },
   "source": [
    "## Step 4: Preprocess - Split Concatenated UUIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9Jkn0JByRarG",
    "outputId": "038d5624-704c-46a1-9180-4780fe329a08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting concatenated UUIDs...\n",
      "✓ Preprocessed: 14,547 lines (808 splits)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "uuid_pattern = r'[a-f0-9]{8}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{12}'\n",
    "\n",
    "print(\"Splitting concatenated UUIDs...\")\n",
    "final_lines = []\n",
    "splits = 0\n",
    "\n",
    "for line in all_lines:\n",
    "    uuid_matches = list(re.finditer(uuid_pattern, line))\n",
    "\n",
    "    if len(uuid_matches) == 0:\n",
    "        final_lines.append(line)\n",
    "    elif len(uuid_matches) == 1:\n",
    "        uuid_start = uuid_matches[0].start()\n",
    "        if uuid_start == 0 or line[uuid_start - 1].isspace():\n",
    "            final_lines.append(line)\n",
    "        else:\n",
    "            splits += 1\n",
    "            final_lines.append(line[:uuid_start].rstrip())\n",
    "            final_lines.append(line[uuid_start:])\n",
    "    else:\n",
    "        # Multiple UUIDs - split at each boundary\n",
    "        splits += 1\n",
    "        for j, match in enumerate(uuid_matches):\n",
    "            uuid_start = match.start()\n",
    "            if j == 0:\n",
    "                if uuid_start > 0 and not line[uuid_start - 1].isspace():\n",
    "                    final_lines.append(line[:uuid_start].rstrip())\n",
    "                if j + 1 < len(uuid_matches):\n",
    "                    final_lines.append(line[uuid_start:uuid_matches[j + 1].start()].rstrip())\n",
    "                else:\n",
    "                    final_lines.append(line[uuid_start:])\n",
    "            else:\n",
    "                if j + 1 < len(uuid_matches):\n",
    "                    final_lines.append(line[uuid_start:uuid_matches[j + 1].start()].rstrip())\n",
    "                else:\n",
    "                    final_lines.append(line[uuid_start:])\n",
    "\n",
    "all_lines = final_lines\n",
    "print(f\"✓ Preprocessed: {len(all_lines):,} lines ({splits} splits)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9JTjLkARarH"
   },
   "source": [
    "## Step 5: Parse Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hr1yOYhxRarI",
    "outputId": "0ec0dc77-9626-4e07-ddc7-45a2e83eb87f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 500...\n",
      "  Processed 1,000...\n",
      "  Processed 1,500...\n",
      "  Processed 2,000...\n",
      "  Processed 2,500...\n",
      "  Processed 3,000...\n",
      "  Processed 3,500...\n",
      "  Processed 4,000...\n",
      "  Processed 4,500...\n",
      "  Processed 5,000...\n",
      "  Processed 5,500...\n",
      "  Processed 6,000...\n",
      "  Processed 6,500...\n",
      "  Processed 7,000...\n",
      "✓ Parsed 7,273 records\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def is_uuid_line(line):\n",
    "    parts = line.split()\n",
    "    if parts:\n",
    "        return bool(re.match(r'^' + uuid_pattern + r'$', parts[0]))\n",
    "    return False\n",
    "\n",
    "records = []\n",
    "i = 0\n",
    "\n",
    "while i < len(all_lines):\n",
    "    try:\n",
    "        line = all_lines[i]\n",
    "\n",
    "        if not is_uuid_line(line):\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        parts = line.split(' ')\n",
    "        devices = parts[6] if len(parts) > 6 else ''\n",
    "        if '/' in devices:\n",
    "            match = re.match(r'(\\d+)', devices)\n",
    "            devices = match.group(1) if match else devices\n",
    "\n",
    "        record = {\n",
    "            'ID': parts[0],\n",
    "            'Org_Name': ' '.join(parts[1:5]),\n",
    "            'Total_Networks_Searched': parts[5] if len(parts) > 5 else '',\n",
    "            'Total_Devices_Searched': devices\n",
    "        }\n",
    "\n",
    "        if i + 1 >= len(all_lines) or is_uuid_line(all_lines[i + 1]):\n",
    "            record.update({\n",
    "                'Time_Frame_Start': '', 'Time_Frame_End': '', 'Reason': '',\n",
    "                'Search_Time': '', 'Search_Type': '', 'Moderation': ''\n",
    "            })\n",
    "            records.append(record)\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        line2 = all_lines[i + 1]\n",
    "        utc_splits = line2.split('UTC')\n",
    "\n",
    "        if len(utc_splits) >= 1:\n",
    "            time_frame_start = utc_splits[0].strip()\n",
    "            if not re.search(r'\\d{2}/\\d{2}/\\d{4}', time_frame_start):\n",
    "                date_in_line1 = re.search(r'(\\d{2}/\\d{2}/\\d{4},)\\s*$', line)\n",
    "                if date_in_line1:\n",
    "                    time_frame_start = date_in_line1.group(1) + ' ' + time_frame_start\n",
    "            record['Time_Frame_Start'] = time_frame_start + ' UTC'\n",
    "        else:\n",
    "            record['Time_Frame_Start'] = ''\n",
    "\n",
    "        if len(utc_splits) >= 2:\n",
    "            end_datetime = utc_splits[1].strip()\n",
    "            datetime_match = re.match(r'(\\d{2}/\\d{2}/\\d{4},\\s+\\d{2}:\\d{2}:\\d{2}\\s+[AP]M)', end_datetime)\n",
    "            record['Time_Frame_End'] = datetime_match.group(1) + ' UTC' if datetime_match else ''\n",
    "        else:\n",
    "            record['Time_Frame_End'] = ''\n",
    "\n",
    "        if len(utc_splits) >= 3:\n",
    "            reason_and_search = utc_splits[2].strip()\n",
    "            search_time_match = re.search(r'(\\d{2}/\\d{2}/\\d{4},\\s+\\d{2}:\\d{2}:\\d{2}\\s+[AP]M)', reason_and_search)\n",
    "            if search_time_match:\n",
    "                record['Reason'] = reason_and_search[:search_time_match.start()].strip()\n",
    "                record['Search_Time'] = search_time_match.group(1) + ' UTC'\n",
    "            else:\n",
    "                record['Reason'] = reason_and_search\n",
    "                record['Search_Time'] = ''\n",
    "        else:\n",
    "            record['Reason'] = ''\n",
    "            record['Search_Time'] = ''\n",
    "\n",
    "        if len(utc_splits) >= 4:\n",
    "            search_type_part = utc_splits[3].strip()\n",
    "            parts = search_type_part.split()\n",
    "            if parts:\n",
    "                record['Search_Type'] = parts[0]\n",
    "                record['Moderation'] = ' '.join(parts[1:]) if len(parts) > 1 else ''\n",
    "            else:\n",
    "                record['Search_Type'] = ''\n",
    "                record['Moderation'] = ''\n",
    "        else:\n",
    "            record['Search_Type'] = ''\n",
    "            record['Moderation'] = ''\n",
    "\n",
    "        records.append(record)\n",
    "        i += 2\n",
    "\n",
    "        if len(records) % 500 == 0:\n",
    "            print(f\"  Processed {len(records):,}...\")\n",
    "    except Exception as e:\n",
    "        i += 1\n",
    "        continue\n",
    "\n",
    "print(f\"✓ Parsed {len(records):,} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ev59Eoa3sIbZ",
    "outputId": "6a4cde9c-a941-4305-a401-a9100c8c212c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting consistent suffix correction...\n",
      "✓ Fixed 51 rows by removing duplicated month suffixes.\n"
     ]
    }
   ],
   "source": [
    "# --- Step 5.5: Consistent Month-Suffix Correction ---\n",
    "print(\"Starting consistent suffix correction...\")\n",
    "\n",
    "corrected_count = 0\n",
    "\n",
    "for record in records:\n",
    "    devices = str(record.get('Total_Devices_Searched', \"\"))\n",
    "    start_date = str(record.get('Time_Frame_Start', \"\"))\n",
    "\n",
    "    # We need at least 2 digits for a month and some device digits to work with\n",
    "    if len(start_date) >= 2 and len(devices) > 2:\n",
    "        # 1. Get the month digits from the start of the date (e.g., \"11\")\n",
    "        # We split by '/' to handle both \"11/\" and \"1/\" cases safely\n",
    "        month_prefix = start_date.split('/')[0]\n",
    "\n",
    "        # Ensure month_prefix is 2 digits if it's 10, 11, or 12\n",
    "        # (The parser consistently grabs 2 digits in the broken rows)\n",
    "        if len(month_prefix) == 2:\n",
    "            # 2. If the device count ends with those exact digits, strip them\n",
    "            if devices.endswith(month_prefix):\n",
    "                record['Total_Devices_Searched'] = devices[:-2]\n",
    "                corrected_count += 1\n",
    "        elif len(month_prefix) == 1:\n",
    "            # Handle single digit months if they are also duplicating (e.g., \"9\")\n",
    "            if devices.endswith(month_prefix):\n",
    "                record['Total_Devices_Searched'] = devices[:-1]\n",
    "                corrected_count += 1\n",
    "\n",
    "print(f\"✓ Fixed {corrected_count} rows by removing duplicated month suffixes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5NUKhoRpRarJ"
   },
   "source": [
    "## Step 6: Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 503
    },
    "id": "PRSMdRTZRarK",
    "outputId": "f627d26b-d67c-4fa1-c9a4-1ebceca8a39d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation:\n",
      "  Total records: 7,273\n",
      "  Valid UUIDs: 7,273\n",
      "  Complete records: 7,273\n",
      "\n",
      "First 3 records:\n",
      "                                     ID               Org_Name  \\\n",
      "0  23d55b89-42eb-4f5c-9281-f5e8bf4864be  Okaloosa County FL SO   \n",
      "1  5720871a-4c6d-4be1-a9c8-6f7524f871f9  Okaloosa County FL SO   \n",
      "2  50bac7fa-a41a-45de-86c8-622fcfa0a0f5  Okaloosa County FL SO   \n",
      "\n",
      "  Total_Networks_Searched Total_Devices_Searched             Time_Frame_Start  \\\n",
      "0                     840                  17652  11/13/2025, 10:00:17 PM UTC   \n",
      "1                     840                  17652  11/13/2025, 10:30:13 PM UTC   \n",
      "2                     840                  17652  11/13/2025, 11:00:22 PM UTC   \n",
      "\n",
      "                Time_Frame_End Reason                  Search_Time  \\\n",
      "0  11/14/2025, 10:00:17 AM UTC   bolo  11/14/2025, 09:53:36 AM UTC   \n",
      "1  11/14/2025, 10:30:13 AM UTC   bolo  11/14/2025, 10:27:32 AM UTC   \n",
      "2  11/14/2025, 11:00:22 AM UTC   bolo  11/14/2025, 10:48:23 AM UTC   \n",
      "\n",
      "  Search_Type Moderation  \n",
      "0      lookup             \n",
      "1      lookup             \n",
      "2      lookup             \n",
      "✓ File saved to: ../exports/RAW-11_1_2025-12_8_2025-Okaloosa_County_FL_SO-Audit_CORRECTED.csv\n",
      "\n",
      "✓ Exported: ../data/RAW-11_1_2025-12_8_2025-Okaloosa_County_FL_SO-Audit_CORRECTED.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "print(\"\\nValidation:\")\n",
    "print(f\"  Total records: {len(df):,}\")\n",
    "print(f\"  Valid UUIDs: {df['ID'].str.match(uuid_pattern, na=False).sum():,}\")\n",
    "print(f\"  Complete records: {((df['Time_Frame_Start'] != '') & (df['Time_Frame_End'] != '') & (df['Search_Time'] != '')).sum():,}\")\n",
    "\n",
    "print(\"\\nFirst 3 records:\")\n",
    "print(df.head(3))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "# 1. Get just the filename without the \"../data/\" prefix\n",
    "file_only = os.path.basename(filename) \n",
    "\n",
    "# 2. Get the name without the \".pdf\" extension\n",
    "name_no_ext = os.path.splitext(file_only)[0]\n",
    "\n",
    "# 3. Construct the new path pointing to the 'exports' folder\n",
    "output_filename = f\"../exports/{name_no_ext}_CORRECTED.csv\"\n",
    "\n",
    "# 4. Save\n",
    "df.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"✓ File saved to: {output_filename}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base_name = os.path.splitext(filename)[0]\n",
    "output_filename = f\"{base_name}_CORRECTED.csv\"\n",
    "df.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"\\n✓ Exported: {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (Okaloosa ALPR)",
   "language": "python",
   "name": "okaloosaalpr_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
